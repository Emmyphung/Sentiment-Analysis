{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README         imdb.vocab     \u001b[34mtest\u001b[m\u001b[m\r\n",
      "Untitled.ipynb imdbEr.txt     \u001b[34mtrain\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tramy/Documents/GitHub/Sentiment_Analysis/train'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "datadir = '/'.join(cwd.split('/')[0:-1]) + '/Sentiment_Analysis/train'\n",
    "datadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = load_files(datadir)\n",
    "# it is not yet a dataframe so cannot use pandas or numpy here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train:<class 'list'>\n",
      "length of text_train = total reviews: 75000\n",
      "text_train[1]:\n",
      "b\"Dan Katzir has produced a wonderful film that takes us on a roller-coaster ride through a real romance set in the troubles surrounding modern Israel.<br /><br />For anyone who's ever been in love, the film brings back the uncertainties, the insecurities and heartache that make love so bitter-sweet. The atmosphere of fear and isolation that came with the difficult times in Israel at that time just serve to intensify the feeling. Instantly, you are drawn in to Dan's plight, and you can't fail to be deeply moved.<br /><br />You can't write drama and passion like this - the contrast between the realities of Dan's desperate, snatched relationship with Iris, and the realities of a state in turmoil make this eminently watchable. If you have an ounce of passion, and have ever been in love, see this film.\"\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train:{}\".format(type(text_train)))\n",
    "print(\"length of text_train = total reviews: {}\".format(len(text_train)))\n",
    "print(\"text_train[3]:\\n{}\".format(text_train[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out some expression, like <br../>\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\"\") for doc in text_train] #we need b\" \" because those are bytes-like objects, not string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500 50000]\n",
      "Sample y_train[1]:\n",
      "[2 2 1]\n"
     ]
    }
   ],
   "source": [
    "print (\"Samples per class (training): {}\".format(np.bincount(y_train)))\n",
    "print (\"Sample y_train[1:4]:\\n{}\".format(y_train[1:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bag of words\n",
    "3 Steps:\n",
    "    1. Tokenize - CountVectorizer\n",
    "    2. Vocab building -  CountVectorizer\n",
    "    3. Encoding - Transform \n",
    "    (Based on how often each word appear. return a vector of 0 and 1, 0: not appear, 1: appear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:n\\  (0, 3192)\t1\n",
      "  (0, 3289)\t1\n",
      "  (0, 5908)\t3\n",
      "  (0, 6720)\t1\n",
      "  (0, 6725)\t1\n",
      "  (0, 7429)\t1\n",
      "  (0, 8512)\t1\n",
      "  (0, 11331)\t1\n",
      "  (0, 12958)\t1\n",
      "  (0, 13471)\t1\n",
      "  (0, 14981)\t1\n",
      "  (0, 16997)\t1\n",
      "  (0, 17381)\t2\n",
      "  (0, 17534)\t1\n",
      "  (0, 18209)\t1\n",
      "  (0, 20529)\t1\n",
      "  (0, 22867)\t1\n",
      "  (0, 23532)\t1\n",
      "  (0, 27359)\t1\n",
      "  (0, 34156)\t1\n",
      "  (0, 37162)\t1\n",
      "  (0, 39661)\t1\n",
      "  (0, 41342)\t2\n",
      "  (0, 42840)\t2\n",
      "  (0, 43154)\t1\n",
      "  :\t:\n",
      "  (74999, 119185)\t4\n",
      "  (74999, 119431)\t1\n",
      "  (74999, 119504)\t1\n",
      "  (74999, 119671)\t1\n",
      "  (74999, 120596)\t1\n",
      "  (74999, 121727)\t1\n",
      "  (74999, 122216)\t1\n",
      "  (74999, 122220)\t1\n",
      "  (74999, 122397)\t3\n",
      "  (74999, 122437)\t1\n",
      "  (74999, 122629)\t1\n",
      "  (74999, 122969)\t1\n",
      "  (74999, 123058)\t1\n",
      "  (74999, 123256)\t4\n",
      "  (74999, 123354)\t1\n",
      "  (74999, 123364)\t1\n",
      "  (74999, 123381)\t1\n",
      "  (74999, 123406)\t1\n",
      "  (74999, 123963)\t1\n",
      "  (74999, 124051)\t1\n",
      "  (74999, 124335)\t6\n",
      "  (74999, 124369)\t1\n",
      "  (74999, 124950)\t1\n",
      "  (74999, 124985)\t1\n",
      "  (74999, 126139)\t3\n"
     ]
    }
   ],
   "source": [
    "#Apply the CountVectorizer function into vectorizing our training data\n",
    "vect = CountVectorizer().fit(text_train) \n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:n\\{}\".format(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:n\\<75000x127229 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 10315468 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train:n\\{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 127229\n",
      "First 20 features:/n['00', '9out', 'ages', 'andress', 'aryeman', 'baio', 'bellah', 'bloodstain', 'briers', 'calm', 'cessation', 'circenses', 'complementing', 'countedsix', 'dahan', 'dench', 'discourages', 'dreamkeeeper', 'elderly', 'esposito', 'fang', 'flaquer', 'frizzyhead', 'gerry', 'grandness', 'halycon', 'hesitated', 'hubiriffic', 'incongruence', 'ireperable', 'journal', 'kindsa', 'landingham', 'limply', 'maaaybbbeee', 'marthe', 'mephestophelion', 'modem', 'mushing', 'nigger', 'oghris', 'oxbridge', 'pensamentos', 'pleaaaaaaaase', 'prettified', 'quantify', 'recommendation', 'retrieving', 'rp', 'scam', 'sequiter', 'sidemen', 'snk', 'sprite', 'stroptomycin', 'swings', 'teoe', 'toiled', 'tsiolkovsky', 'unflagging', 'vaporised', 'wakens', 'wilshire', 'ynis']\n"
     ]
    }
   ],
   "source": [
    "# To look at features in the vocab list - use the get_feature_names() function\n",
    "feature_names = vect.get_feature_names()\n",
    "print (\"Number of features: {}\".format(len(feature_names)))\n",
    "print (\"First 20 features:/n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Once we have the feature, build a classifier using Logistics regression - it works best for high-dimensional sparse data like this. We can take this step to evaluate our feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Next we are trying to improve this accuracy by filtering out super rare words (appears in less than 5 documents) and \n",
    "filter STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 44550\n",
      "First 20 features:/n['00', '000', '001', '007', '00am', '00pm', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '1001']\n"
     ]
    }
   ],
   "source": [
    "#Remove rarely seen features, min-seen-documents = 5\n",
    "vect2 = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect2.transform(text_train)\n",
    "feature_names2 = vect2.get_feature_names()\n",
    "print (\"Number of features: {}\".format(len(feature_names2)))\n",
    "print (\"First 20 features:/n{}\".format(feature_names2[:20]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 44241\n",
      "First 20 features:/n['00', '000', '001', '007', '00am', '00pm', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '1001']\n"
     ]
    }
   ],
   "source": [
    "#Filter stop words\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "vect3 = CountVectorizer(min_df=5, stop_words = \"english\").fit(text_train)\n",
    "feature_names3 = vect3.get_feature_names()\n",
    "print (\"Number of features: {}\".format(len(feature_names3)))\n",
    "print (\"First 20 features:/n{}\".format(feature_names3[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescale data with TF-IDF (term frequency-inverse document frequency)\n",
    "#skip for now but will try again\n",
    "\n",
    "#Laten Dirichlet Allocation for topic summarizing\n",
    "vect_func = CountVectorizer(max_features = 10000, max_df = .15)\n",
    "X = vect_func.fit_transform(text_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation #n_components = number of topics\n",
    "lda = LatentDirichletAllocation(n_components = 10, learning_method =\"batch\", max_iter = 25, random_state = 0)\n",
    "\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mglearn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/01/8d3630ecc767c9de96a9c46e055f2a3a5f9e14a47d3d0348a36a5005fe67/mglearn-0.1.7.tar.gz (540kB)\n",
      "\u001b[K     |████████████████████████████████| 542kB 3.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from mglearn) (1.16.4)\n",
      "Requirement already satisfied: matplotlib in /anaconda3/lib/python3.7/site-packages (from mglearn) (3.1.0)\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.7/site-packages (from mglearn) (0.21.2)\n",
      "Requirement already satisfied: pandas in /anaconda3/lib/python3.7/site-packages (from mglearn) (0.24.2)\n",
      "Requirement already satisfied: pillow in /anaconda3/lib/python3.7/site-packages (from mglearn) (6.1.0)\n",
      "Requirement already satisfied: cycler in /anaconda3/lib/python3.7/site-packages (from mglearn) (0.10.0)\n",
      "Requirement already satisfied: imageio in /anaconda3/lib/python3.7/site-packages (from mglearn) (2.5.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda3/lib/python3.7/site-packages (from matplotlib->mglearn) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda3/lib/python3.7/site-packages (from matplotlib->mglearn) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /anaconda3/lib/python3.7/site-packages (from matplotlib->mglearn) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn->mglearn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda3/lib/python3.7/site-packages (from scikit-learn->mglearn) (0.13.2)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.7/site-packages (from pandas->mglearn) (2019.1)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from cycler->mglearn) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->mglearn) (41.0.1)\n",
      "Building wheels for collected packages: mglearn\n",
      "  Building wheel for mglearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/tramy/Library/Caches/pip/wheels/74/cf/8d/04f4932d15854a36726c6210763c7127e62de28f5c8ddfcf3b\n",
      "Successfully built mglearn\n",
      "Installing collected packages: mglearn\n",
      "Successfully installed mglearn-0.1.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "codswallop    associations  benson        conceals      astro         \n",
      "alleviate     carnality     ddlj          compared      deafening     \n",
      "cuthbert      death         barnett       crude         adrienne      \n",
      "decarlo       1981          barry         bagging       busfield      \n",
      "charlene      1986          decarlo       brands        curfew        \n",
      "decadence     beijing       campier       beijing       caught        \n",
      "charlie       caked         benefiting    carol         circulate     \n",
      "contempt      citations     dat           conceivably   commiserate   \n",
      "angrily       comfy         ddr           baggy         boogie        \n",
      "chad          damnation     casca         casca         accurately    \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "cann          blindingly    1976          damon         bountiful     \n",
      "colour        brannagh      awkwardness   84            angrily       \n",
      "dead          bertrand      believe       dealership    codswallop    \n",
      "decadence     aronofsky     consideration curfew        alleviate     \n",
      "adidas        affectionatelybatalla       attentive     1976          \n",
      "2nd           blitz         basque        catching      beijing       \n",
      "001           com           combines      blare         charlie       \n",
      "1981          decide        consequence   civil         chad          \n",
      "chad          arrangements  contempt      appetizers    1980s         \n",
      "ballroom      chianese      audie         decadence     adulterous    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install mglearn\n",
    "\n",
    "import mglearn\n",
    "sorting = np.argsort(lda.components_, axis = 1)[:, ::-1]\n",
    "feature_names4 = np.array(vect.get_feature_names())\n",
    "\n",
    "mglearn.tools.print_topics(topics = range(10), feature_names= feature_names4, sorting =sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda100 = LatentDirichletAllocation(n_components = 100, learning_method =\"batch\", max_iter = 25, random_state = 0)\n",
    "\n",
    "document_topics = lda100.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       topic 5       topic 6       \n",
      "--------      --------      --------      --------      --------      --------      --------      \n",
      "avengers      comfy         alias         care          compared      cann          caracter      \n",
      "cya           carnality     alyson        bots          bagging       cocoon        decaprio      \n",
      "apologized    chazz         allegations   anger         baggy         acts          amongst       \n",
      "cleaver       1981          churchill     camp          commando      consecutive   collinson     \n",
      "cleavers      afterward     auteurs       coloring      crude         cameron       bleach        \n",
      "conundrum     autos         authoress     advising      convenience   coe           braless       \n",
      "alarming      commiserate   bigtime       daniels       caracter      connor        collaborate   \n",
      "aching        anniversary   buffer        begemot       cahill        connolly      churn         \n",
      "cuthbert      assimilation  appreciative  daines        commenced     anorexic      breakneck     \n",
      "crude         che           alzheimer     cmdr          brethren      charley       danika        \n",
      "batting       death         amateur       debunking     averse        cheadle       composes      \n",
      "agenda        alleviate     churning      blocked       craig         barnum        bumpy         \n",
      "colour        accosted      authenticity  calmed        cat           baroque       bountiful     \n",
      "cuts          bargaining    barrio        brinda        crapness      browsed       bringing      \n",
      "deathbed      boogie        amiable       darbar        decidedly     adherents     carrel        \n",
      "climates      concocts      activities    cielo         commands      consciously   captained     \n",
      "anesthesia    bonehead      adjusted      attempted     courtland     centerfold    auditor       \n",
      "counterbalancecheques       cads          captured      baroque       birkenau      berries       \n",
      "carol         cuckoo        bmw           arne          crudup        47            confided      \n",
      "applicants    astray        arresting     cornfields    bats          actuality     92            \n",
      "\n",
      "\n",
      "topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      \n",
      "bigamist      clutters      darkest       \n",
      "1976          army          darker        \n",
      "curtains      cacophonous   1988          \n",
      "30mins        adrianne      colman        \n",
      "compromise    buffoons      darkly        \n",
      "biggs         babes         bernhardt     \n",
      "bigelow       celebrates    bermuda       \n",
      "biggins       date          buschemi      \n",
      "bauer         casting       appointed     \n",
      "blackness     clutch        databases     \n",
      "compromises   akins         crewed        \n",
      "comrade       cannonball    consequently  \n",
      "consults      chops         bundles       \n",
      "brandt        bum           bogarde       \n",
      "commendably   busily        collides      \n",
      "built         breakers      bigamist      \n",
      "abstracted    cctv          bosnians      \n",
      "afi           catalogs      composers     \n",
      "audie         analytic      casca         \n",
      "cowley        codswallop    contrary      \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda100.components_, axis = 1)[:, ::-1]\n",
    "feature_names5 = np.array(vect.get_feature_names())\n",
    "\n",
    "topics = np.array([7,16,24,25,28,36,37,45,51,53,54,63,89,97])\n",
    "\n",
    "mglearn.tools.print_topics(topics = range(10), feature_names= feature_names4, sorting =sorting, topics_per_chunk=7, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
